{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QB_MqSB11X7",
    "outputId": "0c4f15e6-62cf-4254-86ee-a6ee4da6d8c4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCzVCnA32J7j",
    "outputId": "d23bfd60-ae9f-4be5-d651-41b9226f7609"
   },
   "outputs": [],
   "source": [
    "pip install hmmlearn==0.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFZeN1jK2Jyj",
    "outputId": "d78e5a76-7132-4c80-867d-11416bd91237"
   },
   "outputs": [],
   "source": [
    "pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUVoFsEZ2HFj"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import librosa\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import read\n",
    "import wave\n",
    "from python_speech_features import mfcc\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from scipy.fftpack import dct\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXjh-wYE2RIa",
    "outputId": "899ffc53-7b73-4fe9-e3c0-9f8c3b6c24af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "   #解壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuqQYB-C2UVS"
   },
   "outputs": [],
   "source": [
    "trainDir = 'true_train/' #檔案路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWPjlmHO2XvK",
    "outputId": "aff4f663-290a-492f-80fe-caff7bbe9f50"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7g4utE02ZVy"
   },
   "outputs": [],
   "source": [
    "testDir = 'true_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slRwIPTU2Qjz"
   },
   "outputs": [],
   "source": [
    "#@title 端點偵測 特徵萃取 畫圖\n",
    "import scipy.fftpack as fftpack\n",
    "from scipy.fftpack.realtransforms import dct\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "import numpy as np\n",
    "import sklearn\n",
    "def test1(path,flagl):\n",
    "    #path = \"0.wav\"\n",
    "    f = wave.open(path,\"r\")\n",
    "    # read the wave's format infomation,and return a tuple\n",
    "    params = f.getparams()\n",
    "    # get the info\n",
    "    nchannels, sampwidth, framerate, nframes = params[:4]\n",
    "    # Reads and returns nframes of audio, as a string of bytes. \n",
    "    str_data = f.readframes(nframes)\n",
    "    # close the stream\n",
    "    f.close()\n",
    "    # turn the wave's data to array\n",
    "    wave_data = np.frombuffer(str_data, dtype = np.short)\n",
    "    # for the data is stereo,and format is LRLRLR...\n",
    "    # shape the array to n*2(-1 means fit the y coordinate)\n",
    "    wave_data.shape = -1, 2#(資料自行決定音訊長度,雙通道/多通道(須回原DATASET確定))\n",
    "    # transpose the data\n",
    "    wave_data = wave_data.T\n",
    "    original_wave = wave_data[0]#取單通道做處理\n",
    "    #print(wave_data)\n",
    "\n",
    "    Frame_len = 256\n",
    "    idx = 0\n",
    "    flag = 0\n",
    "    data = []\n",
    "    while(flag == 0):\n",
    "        tmp = 0.0\n",
    "        for i in range(int(idx),int(idx + Frame_len) ):\n",
    "            if idx + Frame_len >= len(original_wave):\n",
    "                flag = 1\n",
    "                tmp += 0\n",
    "            else:\n",
    "                tmp += original_wave[i] ** 2              \n",
    "        data.append(tmp)\n",
    "        idx = idx + Frame_len    \n",
    "\n",
    "    #original_wave = np.array(wave_data) 不太懂這行\n",
    "    #print(original_wave)\n",
    "    k = 3  #可調整\n",
    "    max_frame = max(data)\n",
    "    threshold = 0.075 * max_frame + sum(data[0:k:]) / k\n",
    "    #print(threshold)\n",
    "    \n",
    "\n",
    "    EPD_wave = []\n",
    "    start_idxs = []\n",
    "    end_idxs = []\n",
    "    EPD_wave_start_idxs = []\n",
    "    EPD_wave_end_idxs = []\n",
    "    i = 0\n",
    "    while(i < len(data)):\n",
    "        endure = 1 #能接受幾個沒聲音的音框\n",
    "        # 找出EPD_wave的start_point\n",
    "        while(i < len(data)):\n",
    "            if data[i] > threshold:\n",
    "                start_point = i * Frame_len#實際上是在 *128 的INDEX 因為是這裡的DATA[i]指的是音框\n",
    "                start_idxs.append(start_point)\n",
    "                break\n",
    "            else:\n",
    "                i += 1\n",
    "            i += 1\n",
    "\n",
    "        # 找出EPD_wave的end_point\n",
    "        while(i < len(data)):\n",
    "            if data[i] < threshold:\n",
    "                if endure != 0:\n",
    "                    endure -= 1\n",
    "                    i += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    end_point = i * Frame_len\n",
    "                    end_idxs.append(end_point)\n",
    "                    i += 1\n",
    "                    break\n",
    "            else:\n",
    "                i += 1\n",
    "        #print(start_point,end_point)\n",
    "        # if len(end_idxs) and len(start_idxs) and end_idxs[-1] == start_idxs[-1]:\n",
    "        #     end_idxs.pop()\n",
    "        #     start_idxs.pop()  \n",
    "        #print(EPD_wave)\n",
    "        #print(original_wave[start_point:end_point])\n",
    "        if(len(EPD_wave_start_idxs)==0):\n",
    "          EPD_wave_start_idxs.append(0)\n",
    "          EPD_wave_end_idxs.append(end_point-start_point)\n",
    "        else:\n",
    "          EPD_wave_start_idxs.append(EPD_wave_end_idxs[-1])#加大一點方便看出來 不太行因為會導致後面的一起被 例如第一個只有100 第十個則有1000\n",
    "          EPD_wave_end_idxs.append(EPD_wave_start_idxs[-1]+end_point-start_point)\n",
    "        EPD_wave=np.concatenate([EPD_wave,original_wave[start_point:end_point]])\n",
    "    #print(EPD_wave)\n",
    "        \n",
    "\n",
    "    \n",
    "    #這裡加上 儲存start end point 就好\n",
    "    start_t = np.array(start_idxs)\n",
    "    end_t = np.array(end_idxs)  \n",
    "    EPD_wave_start_t=np.array(EPD_wave_start_idxs)\n",
    "    EPD_wave_end_t=np.array(EPD_wave_end_idxs)\n",
    "    n = 256\n",
    "    m = 0.5\n",
    "\n",
    "    Frame_len = n\n",
    "    overlap = Frame_len * m\n",
    "    idx = 0\n",
    "    flag = 0\n",
    "    frame = []\n",
    "    while(flag == 0):\n",
    "        tmp = []\n",
    "        '''如果這段音框截取的位置沒有超過聲音的長度就直接截取\n",
    "            否則就慢慢複製，不夠的部分補0'''\n",
    "        if idx + Frame_len < len(EPD_wave):\n",
    "            tmp = np.copy(EPD_wave[int(idx) : int(idx+Frame_len)]) # get frame\n",
    "        else:\n",
    "            for i in range(int(idx),int(idx+Frame_len) ):\n",
    "                if idx + Frame_len >= len(EPD_wave):\n",
    "                    flag = 1\n",
    "                    tmp.append(0)\n",
    "                else:\n",
    "                    tmp.append(EPD_wave[i])\n",
    "        tmp = np.array(tmp)                    \n",
    "        frame.append(tmp)\n",
    "        idx = idx + overlap\n",
    "\n",
    "\n",
    "    EPD_wave = np.array(EPD_wave)\n",
    "    EPD_wave = EPD_wave.reshape(-1,)\n",
    "\n",
    "    # Transform list to np.array\n",
    "    frame = np.array(frame) \n",
    "\n",
    "    pre_emphasis = []\n",
    "    for i in range(frame.shape[0]):\n",
    "        tmp = np.append(frame[i][0], frame[i][1:] - 0.95 * frame[i][:-1])\n",
    "        tmp = np.array(tmp)\n",
    "        pre_emphasis.append(tmp)\n",
    "    pre_emphasis = np.array(pre_emphasis)    \n",
    "\n",
    "    hamming_wave = np.hamming(pre_emphasis.shape[1]) * pre_emphasis\n",
    "\n",
    "    NFFT = 512\n",
    "    nfilt = 13\n",
    "    num_ceps = 12\n",
    "    mag_frames = np.absolute(np.fft.rfft(hamming_wave, NFFT))   # fft的幅度\n",
    "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # 功率譜\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = (2595 * np.log10(1 + (framerate / 2) / 700))  # Transfom Hz to Mel\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # 使Mel 刻度間距相等\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))  # 將Mel轉換為Hz\n",
    "    bin = np.floor((NFFT + 1) * hz_points / framerate)\n",
    "\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])   # 左\n",
    "        f_m = int(bin[m])             # 中\n",
    "        f_m_plus = int(bin[m + 1])    # 右\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # 数值穩定性\n",
    "    filter_banks = 20 * np.log10(filter_banks)  # dB\n",
    "    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1 : (num_ceps+1)]\n",
    "    #np.savetxt('mfcc_data.txt',mfcc)\n",
    "    \n",
    "    # MFCC standardization\n",
    "    mfcc = sklearn.preprocessing.StandardScaler().fit_transform(mfcc)\n",
    "\n",
    "    # Compute delta MFCC and delta-delta MFCC\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "    # Concatenate MFCC, delta MFCC and delta-delta MFCC\n",
    "    all_mfcc = np.concatenate([mfcc, delta_mfcc, delta2_mfcc], axis=0)\n",
    "\n",
    "    if flagl == 1:\n",
    "      plt.figure()\n",
    "      plt.suptitle(\"before & after of EPD\", fontsize=20)\n",
    "      plt.subplot(2, 1, 1)\n",
    "      plt.plot(original_wave)\n",
    "      for s, e in zip(start_t, end_t):\n",
    "        #print(s,e)\n",
    "        plt.axvline(x=s, c='#2ca02c') #綠色\n",
    "        plt.axvline(x=e, c='#d62728')  #紅色\n",
    "      plt.subplot(2, 1, 2)\n",
    "      plt.plot(EPD_wave)\n",
    "      for s, e in zip(EPD_wave_start_t, EPD_wave_end_t):\n",
    "        #print(s,e)\n",
    "        plt.axvline(x=s, c='#2ca02c') #綠色\n",
    "        #plt.axvline(x=e, c='#d62728')  #紅色\n",
    "      \n",
    "      plt.show()\n",
    "    return all_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMVs2yGz2Zw7",
    "outputId": "13b82eef-356b-428b-e4c3-46bd11be2469",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fileList = os.listdir(trainDir)\n",
    "trainDataSet = {}\n",
    "flagl = 0\n",
    "for fileName in fileList:\n",
    "    tmp = fileName.split('.')[0]#第幾筆資料         #跟檔名用.分割\n",
    "    label = tmp.split('_')[1]#這裡是實際的label          #跟檔名用_分割\n",
    "    #Edata ,framerate = EPD(trainDir+fileName)\n",
    "    #feature = extract_mfcc(trainDir+fileName,Edata,framerate) \n",
    "    feature = test1(trainDir+fileName,flagl)\n",
    "    print(label)\n",
    "    if label not in trainDataSet.keys():\n",
    "        trainDataSet[label] = []\n",
    "        trainDataSet[label].append(feature)\n",
    "    else:\n",
    "        exist_feature = trainDataSet[label]\n",
    "        exist_feature.append(feature)\n",
    "        trainDataSet[label] = exist_feature\n",
    "print(\"Finish prepare the training data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_trainData = trainDataSet['0']+trainDataSet['1']+trainDataSet['2']+trainDataSet['3']\n",
    "\n",
    "big_trainData = np.vstack(big_trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#有經過離三角濾波器的轉換 Y(M)會比原本的資料來得少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(trainDataSet['3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnviIz15TUmy"
   },
   "outputs": [],
   "source": [
    "def getrate(clus,state,trainDataSet,big_trainData,testDataSet,fileList):\n",
    "    \n",
    "  start_clu =clus #初始群心數\n",
    "  n_clu=clus  #最大群心數\n",
    "\n",
    "  start_states_num = state #隱藏狀態從2開始計算\n",
    "  max_states_num = state\n",
    "  \n",
    "\n",
    "  recognition_rate_array = {}\n",
    "  a = 0 \n",
    "  n_clu_array = {}\n",
    "  b = 0\n",
    "  states_array = {}\n",
    "  d = 0\n",
    "    \n",
    "  for c in range(start_clu,n_clu+1):\n",
    "    kmeans = KMeans(n_clusters=c)#應該是c\n",
    "    kmeans.fit(big_trainData)\n",
    "    centers = kmeans.cluster_centers_\n",
    "  \n",
    "    for g in range(start_states_num,max_states_num+1):\n",
    "      HMM_Models = {}\n",
    "      states_num = g   #設定有多少隱藏狀態\n",
    "      for i in trainDataSet.keys():               #迭代次數 #使用演算法     #準確度的容忍\n",
    "        model = hmm.MultinomialHMM(n_components=states_num,n_iter=20,algorithm='viterbi', tol=0.01)\n",
    "        trainData = trainDataSet[i]\n",
    "        trainData = np.vstack(trainData)\n",
    "        trainData_label = []\n",
    "        for j in range(len(trainData)):\n",
    "          dic_min = np.linalg.norm(trainData[j]-centers[0])\n",
    "          label = 0\n",
    "          for k in range(len(centers)):\n",
    "              if np.linalg.norm(trainData[j]-centers[k])<dic_min:\n",
    "                  dic_min = np.linalg.norm(trainData[j]-centers[k])\n",
    "                  label = k\n",
    "          trainData_label.append(label)\n",
    "              \n",
    "        trainData_label.append(len(centers)-1)  #以群的編號代換特徵值\n",
    "        trainData_label = np.array([trainData_label])\n",
    "        model.fit(trainData_label)\n",
    "        HMM_Models[i] = model\n",
    "\n",
    "      print(\"Finish training of the DHMM models\")\n",
    "\n",
    "\n",
    "\n",
    "      score_cnt = 0\n",
    "      true = []\n",
    "      pred = []\n",
    "      for i in testDataSet.keys():\n",
    "        feature = testDataSet[i]\n",
    "        for h in range(len(feature)):\n",
    "          testData_label = []\n",
    "          for j in range(len(feature[h])):\n",
    "              dic_min = np.linalg.norm(feature[h][j]- centers[0])\n",
    "              label = 0\n",
    "              for k in range(len(centers)):    \n",
    "                  if np.linalg.norm(feature[h][j]-centers[k])<dic_min:\n",
    "                      dic_min = np.linalg.norm(feature[h][j]-centers[k])\n",
    "                      label = k\n",
    "              testData_label.append(label)\n",
    "          scoreList = {}\n",
    "          testData_label = np.array([testData_label])\n",
    "          for model_label in HMM_Models.keys():\n",
    "              model = HMM_Models[model_label]\n",
    "              score = model.score(testData_label)\n",
    "              scoreList[model_label] = score\n",
    "          predict = max(scoreList, key=scoreList.get)\n",
    "          if i==predict:\n",
    "              score_cnt+=1\n",
    "          #print(\"True label is \",i,\"and predict result label is \",predict)\n",
    "          true.append(int(i[i.find('(')+1]))\n",
    "          pred.append(int(predict))\n",
    "\n",
    "      recognition_rate = 100.0 * score_cnt / (len(fileList))\n",
    "      recognition_rate = round(recognition_rate,2)\n",
    "      print(\"Recognition rate is\", recognition_rate, \"%\")\n",
    "      n_clu_array[b] = c\n",
    "      recognition_rate_array[a] = [recognition_rate]\n",
    "      states_array[d]=g\n",
    "      a+=1\n",
    "      b+=1\n",
    "      d+=1\n",
    "      true = np.array(true)\n",
    "      pred = np.array(pred)\n",
    "\n",
    "      # 计算每个类别的召回率\n",
    "      recall_per_class = recall_score(true, pred, average=None)\n",
    "\n",
    "      # 计算 UAR（平均召回率）\n",
    "      uar = np.mean(recall_per_class)\n",
    "\n",
    "      #print(\"UAR is\", uar)\n",
    "      return uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = os.listdir(testDir)\n",
    "testDataSet = {}\n",
    "flag2=1\n",
    "for fileName in fileList:\n",
    "    tmp = fileName.split('.')[0]\n",
    "    label = tmp.split('_')[1]\n",
    "    feature = test1(testDir+fileName,flag2)\n",
    "    flag2=0\n",
    "    if label not in testDataSet.keys():\n",
    "      testDataSet[label] = []\n",
    "      testDataSet[label].append(feature)\n",
    "    else:\n",
    "      exist_feature = testDataSet[label]\n",
    "      exist_feature.append(feature)\n",
    "      testDataSet[label] = exist_feature\n",
    "print(\"Finish prepare the testing data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3IbwuRTqUWq5"
   },
   "source": [
    "### **3d圖**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJ7-SqY0KDyY",
    "outputId": "7511216d-260e-4059-8ad5-2670e0ddf3ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_clu = 1 #初始群心數\n",
    "n_clu= 100  #最大群心數\n",
    "\n",
    "start_states_num = 1 #隱藏狀態從2開始計算\n",
    "max_states_num = 100\n",
    "#固定是1\n",
    "cluj=10   #群心一次跳幾個\n",
    "statej=10  #隱藏狀態一次跳幾個\n",
    "\n",
    "all_array = [[0 for x in range(max_states_num+statej)] for y in range(n_clu+cluj)]  \n",
    "               #columns                 #rows\n",
    "\n",
    "recognition_rate_array = {}\n",
    "a = 0 \n",
    "n_clu_array = {}\n",
    "b = 0\n",
    "states_array = {}\n",
    "d = 0\n",
    "\n",
    "\n",
    "\n",
    "fileList = os.listdir(testDir)\n",
    "for c in range(start_clu,n_clu+1,cluj):\n",
    "  kmeans = KMeans(n_clusters=c)#應該是c\n",
    "  kmeans.fit(big_trainData)\n",
    "  centers = kmeans.cluster_centers_\n",
    " \n",
    "  for g in range(start_states_num,max_states_num+1,statej):\n",
    "    print(c,g)\n",
    "    HMM_Models = {}\n",
    "    states_num = g   #設定有多少隱藏狀態\n",
    "    for i in trainDataSet.keys():               #迭代次數 #使用演算法     #準確度的容忍\n",
    "      model = hmm.MultinomialHMM(n_components=states_num,n_iter=20,algorithm='viterbi', tol=0.01)\n",
    "      trainData = trainDataSet[i]\n",
    "\n",
    "      trainData = np.vstack(trainData)\n",
    "      trainData_label = []\n",
    "      for j in range(len(trainData)):\n",
    "        dic_min = np.linalg.norm(trainData[j]-centers[0])\n",
    "        label = 0\n",
    "        for k in range(len(centers)):\n",
    "            if np.linalg.norm(trainData[j]-centers[k])<dic_min:\n",
    "                dic_min = np.linalg.norm(trainData[j]-centers[k])\n",
    "                label = k\n",
    "        trainData_label.append(label)\n",
    "            \n",
    "      trainData_label.append(len(centers)-1)  #以群的編號代換特徵值\n",
    "      trainData_label = np.array([trainData_label])\n",
    "      model.fit(trainData_label)\n",
    "      HMM_Models[i] = model\n",
    "\n",
    "    print(\"Finish training of the DHMM models\")\n",
    "\n",
    "    score_cnt = 0\n",
    "    true = []\n",
    "    pred = []\n",
    "    for i in testDataSet.keys():\n",
    "      feature = testDataSet[i]\n",
    "      for h in range(len(feature)):\n",
    "        testData_label = []\n",
    "        for j in range(len(feature[h])):\n",
    "            dic_min = np.linalg.norm(feature[h][j]- centers[0])\n",
    "            label = 0\n",
    "            for k in range(len(centers)):    \n",
    "                if np.linalg.norm(feature[h][j]-centers[k])<dic_min:\n",
    "                    dic_min = np.linalg.norm(feature[h][j]-centers[k])\n",
    "                    label = k\n",
    "            testData_label.append(label)\n",
    "        scoreList = {}\n",
    "        testData_label = np.array([testData_label])\n",
    "        for model_label in HMM_Models.keys():\n",
    "            model = HMM_Models[model_label]\n",
    "            score = model.score(testData_label)\n",
    "            scoreList[model_label] = score\n",
    "        predict = max(scoreList, key=scoreList.get)\n",
    "        if i==predict:\n",
    "            score_cnt+=1\n",
    "        #print(\"True label is \",i,\"and predict result label is \",predict)\n",
    "        true.append(int(i[i.find('(')+1]))\n",
    "        pred.append(int(predict))\n",
    "\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    # 计算每个类别的召回率\n",
    "    recall_per_class = recall_score(true, pred, average=None)\n",
    "\n",
    "    # 计算 UAR（平均召回率）\n",
    "    uar = np.mean(recall_per_class)\n",
    "    recognition_rate = 100.0 * score_cnt / (len(fileList))\n",
    "    recognition_rate = round(recognition_rate,2)\n",
    "    print(\"Recognition rate is\", recognition_rate, \"%\")\n",
    "    n_clu_array[b]=c\n",
    "    recognition_rate_array[a] = [uar]\n",
    "    states_array[d]=g\n",
    "    a+=1\n",
    "    b+=1\n",
    "    d+=1\n",
    "    all_array[c][g]=uar\n",
    "    print(all_array[c][g])\n",
    "    print(\"UAR is\", uar)\n",
    "print(\"ok\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htHF9sFhPDT3",
    "outputId": "ef8e3c3a-3e4d-4fca-b7b5-83966ea26719"
   },
   "outputs": [],
   "source": [
    "recognition_rate_tenford=[]\n",
    "n_clu_tenford=[]\n",
    "states_tenford=[]\n",
    "#print(len(recognition_rate_array))\n",
    "for i in range(len(recognition_rate_array)):\n",
    "  recognition_rate_tenford.append(recognition_rate_array[i][0])\n",
    "  n_clu_tenford.append(n_clu_array[i])\n",
    "  # print(i)\n",
    "  states_tenford.append(states_array[i])   \n",
    "import copy \n",
    "m = recognition_rate_tenford\n",
    "t = copy.deepcopy(m)\n",
    "# 求m个最大的数值及其索引\n",
    "max_number = []\n",
    "max_num_index_list = []\n",
    "for _ in range(5):\n",
    "    number = max(t)\n",
    "    index = t.index(number)\n",
    "    t[index] = 0\n",
    "    max_number.append(number)\n",
    "    max_num_index_list.append(index)\n",
    "t = []\n",
    "print(max_number)\n",
    "print(max_num_index_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenford=\"true_tenfordset/\"\n",
    "flag=0\n",
    "BigTrainDataSet={}\n",
    "for i in range(1,11):\n",
    "    Train = {}\n",
    "    FileList = os.listdir(tenford+str(i-1)+\"/\")\n",
    "    for fileName in FileList:\n",
    "      tmp = fileName.split('.')[0]#第幾筆資料         #跟檔名用.分割\n",
    "      label = tmp.split('_')[1]#這裡是實際的label          #跟檔名用_分割\n",
    "      feature = test1(tenford+str(i-1)+\"/\"+fileName,flag)    \n",
    "      if label not in Train.keys():\n",
    "          Train[label] = []\n",
    "          Train[label].append(feature)\n",
    "      else:\n",
    "          exist_feature = Train[label]\n",
    "          exist_feature.append(feature)\n",
    "          Train[label] = exist_feature\n",
    "    BigTrainDataSet[i]=Train\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BigTrainDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[[0]*(10) for i in range(5)]\n",
    "flag=0\n",
    "#print(n_clu_array[max_num_index_list[0]])\n",
    "for i in range(1,11):\n",
    "    TrainDataSet = {}\n",
    "    TestDataSet = {}\n",
    "    for tt in range(4):\n",
    "        TrainDataSet[str(tt)]=[]\n",
    "        TestDataSet[str(tt)]=[]\n",
    "    for k in range(1,11):\n",
    "        if(k==i):\n",
    "            continue\n",
    "        TrainDataSet['0']+=BigTrainDataSet[k]['0']\n",
    "        TrainDataSet['1']+=BigTrainDataSet[k]['1']\n",
    "        TrainDataSet['2']+=BigTrainDataSet[k]['2']\n",
    "        TrainDataSet['3']+=BigTrainDataSet[k]['3']\n",
    "    TrainData = TrainDataSet['0']+TrainDataSet['1']+TrainDataSet['2']+TrainDataSet['3']\n",
    "    TrainData = np.vstack(TrainData)\n",
    "    TestDataSet['0']+=BigTrainDataSet[i]['0']\n",
    "    TestDataSet['1']+=BigTrainDataSet[i]['1']\n",
    "    TestDataSet['2']+=BigTrainDataSet[i]['2']\n",
    "    TestDataSet['3']+=BigTrainDataSet[i]['3']\n",
    "      #print(\"Finish prepare the testing data\")\n",
    "    FILELIST=os.listdir(tenford+str(i-1)+\"/\")\n",
    "    for j in range(5):#計得要改回11\n",
    "    #print(now)\n",
    "      arr[j][i-1]=getrate(n_clu_tenford[max_num_index_list[j]],states_tenford[max_num_index_list[j]],TrainDataSet,TrainData,TestDataSet,FILELIST)\n",
    "    print(arr)\n",
    "  #記得要改 i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fuD8WXl3PlX",
    "outputId": "f6fc8b20-a7e1-4f96-b5d7-080f775f4a59"
   },
   "outputs": [],
   "source": [
    "taveragerate=[0]*(5)\n",
    "for i in range(5):\n",
    "  now=0\n",
    "  for j in range(1,11):#計得要改回11\n",
    "    now+=arr[i][j-1]\n",
    "  taveragerate[i]=now/10\n",
    "best=taveragerate.index(max(taveragerate))\n",
    "print(arr)\n",
    "print(taveragerate)\n",
    "print(\"最佳的隱藏狀態和分群數量為\"+str(n_clu_tenford[max_num_index_list[best]])+\" \"+str(states_tenford[max_num_index_list[best]]))\n",
    "print(states_tenford)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(max_num_index_list)):\n",
    "    print(n_clu_tenford[max_num_index_list[i]],states_tenford[max_num_index_list[i]])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
